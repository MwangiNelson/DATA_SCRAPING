{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nelsonn/miniconda3/envs/kaleidos/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from typing import List,Tuple\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GPT 3.5-turbo labeller**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_classifier(text:str, classify_by:str, labels:List[str], api_key:str, temperature:float = 0.0) -> str:\n",
    "    \"\"\"\n",
    "    Uses OpenAIs GPT-3.5-turbo model to classify texts. Visit https://platform.openai.com/docs/models/gpt-3-5 for full documentation \n",
    "\n",
    "    @param text: text you want to extract information from\n",
    "    @param classify_by: keyword to extract\n",
    "    @param labels: labels to choose from\n",
    "    @param api_key: OpenAI API key\n",
    "    @param temperature: OpenAI parameter: Higher values means the model will take more risks. E.g. 0.9 for more creative applications, and 0 for ones with a well-defined answer.\n",
    "    @return: The found label\n",
    "    \"\"\"\n",
    "    openai.api_key = api_key\n",
    "    response = None  # Define response with a default value\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": f\"You are a classification assistant and need to classify texts based on their {classify_by}. You may only return one of these labels: {', '.join(labels)}. \\\n",
    "                                Return nothing except one of the mentioned labels. The output should only contain a single word.\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Text to classify: {text}\",\n",
    "                },\n",
    "            ],\n",
    "            temperature=0,\n",
    "        )\n",
    "        answer = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        return {\"result\": answer}\n",
    "    except Exception as e: \n",
    "        return f\"That didn't work. Got error: {e} and message: {response}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ↑ necessary bricks function \n",
    "# -----------------------------------------------------------------------------------------\n",
    "# ↓ example implementation \n",
    "\n",
    "def example_integration():\n",
    "\n",
    "    texts = [\"Can I help or reach out in any way? I worry she may be all alone.\", \"I just feel like I'm falling apart.\" ,\"My body is gross now with tubing sticking out, I feel awful all the time.\"]\n",
    "    api_key = os.getenv('OPENAI_API_KEY') # paste your OpenAI API key here\n",
    "    classify_by = \"medical condition\"\n",
    "    labels = [\"anorexic\", \"not anorexic\"]\n",
    "    \n",
    "    for text in texts:\n",
    "        print(f\"the {classify_by} of \\\"{text}\\\" is {gpt_classifier(text, classify_by, labels, api_key)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the medical condition of \"Can I help or reach out in any way? I worry she may be all alone.\" is That didn't work. Did you provide a valid API key? Got error: You exceeded your current quota, please check your plan and billing details. and message: None\n",
      "the medical condition of \"I just feel like I'm falling apart.\" is That didn't work. Did you provide a valid API key? Got error: You exceeded your current quota, please check your plan and billing details. and message: None\n",
      "the medical condition of \"My body is gross now with tubing sticking out, I feel awful all the time.\" is That didn't work. Did you provide a valid API key? Got error: You exceeded your current quota, please check your plan and billing details. and message: None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "example_integration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "print(os.getenv('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LABELLING USING BERT-uncased**\n",
    "\n",
    "Imma use BERT-base-uncased to label my data. ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize input\n",
    "texts = [\"Can I help or reach out in any way? I worry she may be all alone.\", \"I just feel like I'm falling apart.\" ,\"My body is gross now with tubing sticking out, I feel awful all the time.\"]\n",
    "\n",
    "inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.6299, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.3173, -0.2980],\n",
      "        [ 0.2364, -0.2202],\n",
      "        [ 0.2164, -0.1904]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have labels (0 for \"not anorexic\", 1 for \"anorexic\")\n",
    "labels = torch.tensor([0, 1,0])  # Example labels\n",
    "\n",
    "outputs = model(**inputs, labels=labels)\n",
    "loss = outputs.loss\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not anorexic', 'not anorexic', 'not anorexic']\n"
     ]
    }
   ],
   "source": [
    "# Apply softmax to get probabilities\n",
    "probs = F.softmax(outputs.logits, dim=1)\n",
    "\n",
    "# Get the predicted labels\n",
    "predicted_labels = torch.argmax(probs, dim=1)\n",
    "\n",
    "# Define class labels\n",
    "class_labels = ['not anorexic', 'anorexic']\n",
    "\n",
    "# Map predicted labels to class names\n",
    "predicted_classes = [class_labels[label] for label in predicted_labels]\n",
    "\n",
    "print(predicted_classes)\n",
    "\n",
    "# This definitely doesn't do what I want it to do"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaleidos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
